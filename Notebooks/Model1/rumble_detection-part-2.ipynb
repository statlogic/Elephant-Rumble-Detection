{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "eYXzo-MKN0Dx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import decimal\n",
    "import math\n",
    "from scipy.fftpack import dct\n",
    "from scipy.signal.windows import hann\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "pv7R0hkxN9Zv"
   },
   "outputs": [],
   "source": [
    "# Round function\n",
    "def round_half_up(number):\n",
    "  return int(decimal.Decimal(number).quantize(decimal.Decimal('1'), rounding=decimal.ROUND_HALF_UP))\n",
    "\n",
    "# Return necessary variables to correlate Greenwood scale with Hertz\n",
    "def returnValueTuple():\n",
    "  fmin = 10\n",
    "  fmax = 10000\n",
    "  k = 0.88\n",
    "  A = fmin/(1-k)\n",
    "  a = math.log10(fmax/A+k)\n",
    "  return (a,A,k)\n",
    "\n",
    "# Retreive Greenwood from Hertz\n",
    "def greenwood_from_hertz(hertz):\n",
    "  a,A,k = returnValueTuple()\n",
    "  f = hertz\n",
    "  return (1/a)*math.log10(f/A+k)\n",
    "\n",
    "# Retreive Hertz from Greenwood\n",
    "def hertz_from_greenwood(greenwood):\n",
    "  a,A,k = returnValueTuple()\n",
    "  fp = greenwood\n",
    "  return A*(10**(a*fp)-k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFFT: The size of FFT, It FFT number is less the precision will be less because of dropping many samples; \n",
    "#       Using large FFT allows zero padding which is neutral when frequency domain is concerened\n",
    "#       Here Multiplication by 2 is done which allows some padding but is safe as no sample are dropped \n",
    "def calculateNFFTS(sampleRate,windowLength):\n",
    "  windowLengthSample = windowLength*sampleRate\n",
    "  nfft = 1\n",
    "  while nfft < windowLengthSample:\n",
    "    nfft*=2\n",
    "  return nfft\n",
    "\n",
    "# Pre Emphasis: Amplification of high frequencies to balance the frequency spectrum as high frequencies tend to have low magnitude when compared to lower ones\n",
    "# Formula: x`(n) = x(n) - a*x(n-1) where a = pre-emphasis factor\n",
    "def preEmphasis(signal,preEmphasisFactor=0.97):\n",
    "  return np.append(signal[0], signal[1:] - preEmphasisFactor * signal[:-1])\n",
    "\n",
    "# Numpy stride technique, refer: \n",
    "# https://ellisvalentiner.com/post/np-strides-trick/ and\n",
    "# https://ipython-books.github.io/46-using-stride-tricks-with-numpy/\n",
    "def rollingWindow(paddedSignal,frameLength,step=1):\n",
    "    shape = paddedSignal.shape[:-1] + (paddedSignal.shape[-1] - frameLength + 1, frameLength)\n",
    "    strides = paddedSignal.strides + (paddedSignal.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(paddedSignal, shape=shape, strides=strides)[::step]\n",
    "\n",
    "# Cut the signal in frames, frameStep = Number of frames of previous sample after which 2nd sample frames should start, strideTrick uses internal numpy method\n",
    "def frameSignal(signal, frameLength, frameStep, windowFunction):\n",
    "\n",
    "  lengthOfSignal = len(signal)\n",
    "  frameLength = int(round_half_up(frameLength))\n",
    "  frameStep = int(round_half_up(frameStep))\n",
    "\n",
    "  if lengthOfSignal < frameLength:\n",
    "    numberOfFrames = 1\n",
    "  else:\n",
    "    numberOfFrames = 1 + int(math.ceil((1.0 * lengthOfSignal - frameLength) / frameStep))\n",
    "\n",
    "  paddingLength = int((numberOfFrames - 1) * frameStep + frameLength)\n",
    "  zeros = np.zeros((paddingLength - lengthOfSignal,))\n",
    "  paddedsignal = np.concatenate((signal, zeros))\n",
    "\n",
    "  # Use numpy to efficiently make windows\n",
    "  window = windowFunction(frameLength)\n",
    "  frames = rollingWindow(paddedsignal,frameLength,step = frameStep)\n",
    "\n",
    "  return frames * window\n",
    "\n",
    "# Magnitude Spectrum: Calculated using numpy.fft.rfft' returns the absolute values after taking fourier transfrom\n",
    "def magSpectrum(frames,nfft):\n",
    "  if np.shape(frames)[1] > nfft:\n",
    "    logging.warn('frame length {} is greater than FFT size {}, frame will be truncated. Increase NFFT to avoid.'.format(np.shape(frames)[1],nfft))\n",
    "  complexSpectrum = np.fft.rfft(frames,nfft)\n",
    "  return np.absolute(complexSpectrum)\n",
    "\n",
    "# Power Spectrum: Calulated using 1/n*|si|^2 where 'si' is fourier transform\n",
    "def powSpectrum(frames,nfft):\n",
    "  return 1.0/nfft*np.square(magSpectrum(frames,nfft))\n",
    "\n",
    "# Log power spectrum and normalize: Not needed\n",
    "def logPowSpectrum(frames,nfft,normalize = 1):\n",
    "  powerSpectrum = powSpectrum(frames,nfft)\n",
    "  # If values are close to 0 or 0 log function will have problem\n",
    "  powerSpectrum[powerSpectrum<= 1e-30] = 1e-30\n",
    "  logPowerSpectrum = 10*np.log10(powerSpectrum)\n",
    "  if normalize:\n",
    "    return logPowerSpectrum - np.max(logPowerSpectrum)\n",
    "  else:\n",
    "    return logPowerSpectrum\n",
    "\n",
    "# Returns Filter Banks: \n",
    "# Filters correspond to rows and Columns correspond to fft bins\n",
    "# returns: array of size = numberOfFilter*(nfft/2+1)\n",
    "def returnFilterBanks(numberOfFilter=30,nfft=512,sampleRate=16000,lowFrequency=0,highFrequency=None):\n",
    "  highFrequency= highFrequency or sampleRate/2\n",
    "  assert highFrequency <= sampleRate/2, \"High Frequency is greater than sampleRate/2\"\n",
    "\n",
    "  # Calculate evenly spaced pts in greenwood\n",
    "  lowGreenWood = greenwood_from_hertz(lowFrequency)\n",
    "  highGreenWood = greenwood_from_hertz(highFrequency)\n",
    "  greenWoodPoints = np.linspace(lowGreenWood,highGreenWood,numberOfFilter+2)\n",
    "\n",
    "  # Current points are in Hertz, but we use fft bins, so we have to convert from Hertz to fft bin number\n",
    "  bin = np.floor((nfft+1)*hertz_from_greenwood(greenWoodPoints)/sampleRate)\n",
    "\n",
    "  fbank = np.zeros([numberOfFilter,nfft//2+1])\n",
    "  for j in range(0,numberOfFilter):\n",
    "      for i in range(int(bin[j]), int(bin[j+1])):\n",
    "          fbank[j,i] = (i - bin[j]) / (bin[j+1]-bin[j])\n",
    "      for i in range(int(bin[j+1]), int(bin[j+2])):\n",
    "          fbank[j,i] = (bin[j+2]-i) / (bin[j+2]-bin[j+1])\n",
    "  return fbank\n",
    "\n",
    "# Array of banpass filters(filters which decide what frequencies can pass)\n",
    "def filterBank(signal,sampleRate=16000, windowLength=0.025, windowStep=0.01, numberOfFilter=30, nfft=512,\n",
    "               lowFrequency=0, highFrequency=None, preEmphasisFactor=0.97, windowFunction=lambda x: np.ones((x,))):\n",
    "  \n",
    "  highFrequency = highFrequency or sampleRate/2\n",
    "  \n",
    "  # Step 1: Apply pre emphasis on signal\n",
    "  signal = preEmphasis(signal,preEmphasisFactor)\n",
    "\n",
    "  # Step 2: Frame a signal to overlapping signals: 2d nd array\n",
    "  frames = frameSignal(signal, windowLength*sampleRate, windowStep*sampleRate, windowFunction)\n",
    "\n",
    "  # Step 3: Calculate the power spectrum\n",
    "  powerSpec = powSpectrum(frames,nfft)\n",
    "  \n",
    "  # Step 4: Calculate the total energy in each frame\n",
    "  energy = np.sum(powerSpec,1)\n",
    "  energy = np.where(energy==0,np.finfo(float).eps,energy)\n",
    "\n",
    "  # Step 5: Get Filter Banks and its energy\n",
    "  filterBank = returnFilterBanks(numberOfFilter,nfft,sampleRate,lowFrequency,highFrequency)\n",
    "  features = np.dot(powerSpec,filterBank.T)\n",
    "  features = np.where(features==0,np.finfo(float).eps,features)\n",
    "\n",
    "  return features,energy\n",
    "\n",
    "# Apply Sinosoidal liftering: Increases the magnitude of highFrequency DCT coefficients\n",
    "# To de-emphasize higher MFCCs which has been claimed to improve audio detection under noisy circumstances \n",
    "def lifter(cepstra,lifterParameter=22):\n",
    "  L = lifterParameter\n",
    "  if L > 0:\n",
    "    nframes,ncoeff = np.shape(cepstra)\n",
    "    n = np.arange(ncoeff)\n",
    "    lift = 1 + (L/2.)*np.sin(np.pi*n/L)\n",
    "    return lift*cepstra\n",
    "  else:\n",
    "    return cepstra # No liftering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gfcc(signal,sampleRate = 16000, windowLength = 0.025, windowStep = 0.01, returnCepstrumNumber = 13, \n",
    "         numberOfFilter = 30, nfft = None, lowFrequency = 0, highFrequency = None, preEmphasisFactor = 0.97, \n",
    "         lifterParameter = 22, appendEnergy = True, windowFunction = lambda x: np.ones((x,))):\n",
    "  \n",
    "  # Step 1: Calculate the length of analysis window\n",
    "  nfft = nfft or calculateNFFTS(sampleRate,windowLength)\n",
    "\n",
    "  # Step 2 + 3: Calculate Greenwood Filter Bank and apply to power spectrum\n",
    "  features, energy = filterBank(signal,sampleRate, windowLength,windowStep, numberOfFilter, nfft, lowFrequency, highFrequency, preEmphasisFactor, windowFunction)\n",
    "\n",
    "  # Step 4: Take log of features\n",
    "  features = np.log(features)\n",
    "\n",
    "  # Step 5: Take discrete fourier transform\n",
    "  features = dct(features,axis=1,norm='ortho')[:,:returnCepstrumNumber]\n",
    "\n",
    "  # Step 6: Liftering\n",
    "  features = lifter(features,lifterParameter)\n",
    "\n",
    "  # First frame generally defines energy of that frame\n",
    "  if appendEnergy:\n",
    "    features[:,0] = np.log(energy)\n",
    "  \n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = '../mubin/Data/Training'\n",
    "testing_data_path = '../mubin/Data/Testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted files\n",
    "\n",
    "for path in [testing_data_path,training_data_path]:\n",
    "    for folder in ['Noise','Rumbles']:\n",
    "        for file in ['.amlignore', '.amlignore.amltmp']:\n",
    "            if os.path.exists(path+'/'+folder+'/'+file):\n",
    "                os.remove(path+'/'+folder+'/'+file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "qYFFcxEPQUl6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3180\n",
      "3180\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "noiseFiles = next(os.walk(training_data_path+'/Noise'))[2]\n",
    "print(len(noiseFiles))\n",
    "\n",
    "rumbleFiles = next(os.walk(training_data_path+'/Rumbles'))[2]\n",
    "print(len(rumbleFiles))\n",
    "\n",
    "print(len(noiseFiles)==len(rumbleFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6360/6360 [09:28<00:00, 11.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# 4sec*2000 sr = 8000\n",
    "target = 8000\n",
    "targetSampleRate = 2000\n",
    "\n",
    "def returnGfccCoeff(audio,sample_rate):\n",
    "    gfccs = gfcc(audio,sample_rate,windowFunction=hann,returnCepstrumNumber=18,preEmphasisFactor=0,lifterParameter=0,lowFrequency=10,highFrequency=500,windowLength=0.075,windowStep=0.025)\n",
    "    return gfccs\n",
    "    \n",
    "def preProcess(fileName,ar,fileType):\n",
    "    audio,sample_rate = librosa.load(fileName,sr=targetSampleRate)\n",
    "    if len(audio)>=target:\n",
    "        newAudio = audio[:target]\n",
    "        ar.append([returnGfccCoeff(newAudio,targetSampleRate),fileType])\n",
    "    elif len(audio)>=4000:\n",
    "        newAudio = np.append(audio,np.zeros(target-len(audio)))\n",
    "        ar.append([returnGfccCoeff(newAudio,targetSampleRate),fileType])\n",
    "    \n",
    "noise_set_size = len(noiseFiles)\n",
    "rumble_set_size = len(rumbleFiles)\n",
    "noiseList = [['Noise/noise_'+str(i)+'.wav','noise'] for i in range(1,noise_set_size+1)]\n",
    "rumbleList = [['Rumbles/rumble_'+str(i)+'.wav','rumble'] for i in range(1,rumble_set_size+1)]\n",
    "finalList = noiseList+rumbleList\n",
    "    \n",
    "preProcessedTrainFiles = []\n",
    "for i in tqdm(finalList):\n",
    "    filePath = i[0]\n",
    "    fileType = i[1]\n",
    "    fileName = training_data_path+'/'+filePath\n",
    "    preProcess(fileName,preProcessedTrainFiles,fileType)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-4.353478744308409, 8.847299193587524, 3.689...</td>\n",
       "      <td>noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-5.1132014529120475, 4.239382867939322, 0.93...</td>\n",
       "      <td>noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-4.634898580352101, 6.435720899107354, 4.121...</td>\n",
       "      <td>noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-4.912678649511506, 8.761000676590456, 2.236...</td>\n",
       "      <td>noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-4.104406307550257, 8.320791089854463, 3.938...</td>\n",
       "      <td>noise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature  class\n",
       "0  [[-4.353478744308409, 8.847299193587524, 3.689...  noise\n",
       "1  [[-5.1132014529120475, 4.239382867939322, 0.93...  noise\n",
       "2  [[-4.634898580352101, 6.435720899107354, 4.121...  noise\n",
       "3  [[-4.912678649511506, 8.761000676590456, 2.236...  noise\n",
       "4  [[-4.104406307550257, 8.320791089854463, 3.938...  noise"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features_df=pd.DataFrame(preProcessedTrainFiles,columns=['feature','class'])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158, 18)    6054\n",
       "dtype: int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features_df.apply(lambda row: row['feature'].shape,axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6054, 158, 18) (6054,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder=LabelEncoder()\n",
    "y_train = to_categorical(labelencoder.fit_transform(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4843, 158, 18) (4843, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = to_categorical(labelencoder.transform(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1211, 158, 18) (1211, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1,X_train.shape[1],X_train.shape[2],1)\n",
    "\n",
    "X_val = X_val.reshape(-1,X_val.shape[1],X_val.shape[2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4843, 158, 18, 1) (4843, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1211, 158, 18, 1) (1211, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757\n",
      "757\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "noiseTestFiles = next(os.walk(testing_data_path+'/Noise'))[2]\n",
    "print(len(noiseTestFiles))\n",
    "\n",
    "rumbleTestFiles = next(os.walk(testing_data_path+'/Rumbles'))[2]\n",
    "print(len(rumbleTestFiles))\n",
    "\n",
    "print(len(noiseTestFiles)==len(rumbleTestFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1514/1514 [02:26<00:00, 10.35it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "noise_test_set_size = len(noiseTestFiles)\n",
    "rumble_test_set_size = len(rumbleTestFiles)\n",
    "\n",
    "noiseTestList = [['Noise/noise_'+str(i)+'.wav','noise'] for i in range(1,noise_test_set_size+1)]\n",
    "rumbleTestList = [['Rumbles/rumble_'+str(i)+'.wav','rumble'] for i in range(1,rumble_test_set_size+1)]\n",
    "\n",
    "finalTestList = noiseTestList+rumbleTestList\n",
    "    \n",
    "preProcessedTestFiles = []\n",
    "for i in tqdm(finalTestList):\n",
    "    filePath = i[0]\n",
    "    fileType = i[1]\n",
    "    fileName = testing_data_path+'/'+filePath\n",
    "    preProcess(fileName,preProcessedTestFiles,fileType)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-5.0897777786499985, 3.8598030465252444, 1.6...</td>\n",
       "      <td>noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-4.6996006670551616, 7.3385415086445285, 1.8...</td>\n",
       "      <td>noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-5.3567076987659465, 5.48035718638847, 5.174...</td>\n",
       "      <td>noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-3.577061882726914, 2.1885531707106054, 0.56...</td>\n",
       "      <td>noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-3.911528384184917, 0.6352179563304279, 0.77...</td>\n",
       "      <td>noise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature  class\n",
       "0  [[-5.0897777786499985, 3.8598030465252444, 1.6...  noise\n",
       "1  [[-4.6996006670551616, 7.3385415086445285, 1.8...  noise\n",
       "2  [[-5.3567076987659465, 5.48035718638847, 5.174...  noise\n",
       "3  [[-3.577061882726914, 2.1885531707106054, 0.56...  noise\n",
       "4  [[-3.911528384184917, 0.6352179563304279, 0.77...  noise"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_test_features_df = pd.DataFrame(preProcessedTestFiles,columns=['feature','class'])\n",
    "extracted_test_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158, 18)    1462\n",
       "dtype: int64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_test_features_df.apply(lambda row: row['feature'].shape,axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.array(extracted_test_features_df['feature'].tolist())\n",
    "y_test=np.array(extracted_test_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = to_categorical(labelencoder.transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(-1,X_test.shape[1],X_test.shape[2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1462, 158, 18, 1) (1462, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D, Dense, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, input_shape=X_train.shape[1:] ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 156, 16, 64)       640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 156, 16, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 156, 16, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 78, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 76, 6, 32)         18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 76, 6, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 76, 6, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 38, 3, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 3648)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                116768    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 136,450\n",
      "Trainable params: 136,194\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4843 samples, validate on 1211 samples\n",
      "Epoch 1/100\n",
      "4843/4843 [==============================] - 12s 3ms/sample - loss: 0.1605 - accuracy: 0.9440 - val_loss: 0.1541 - val_accuracy: 0.9678\n",
      "Epoch 2/100\n",
      "4843/4843 [==============================] - 12s 2ms/sample - loss: 0.0847 - accuracy: 0.9719 - val_loss: 0.0999 - val_accuracy: 0.9711\n",
      "Epoch 3/100\n",
      "4843/4843 [==============================] - 12s 2ms/sample - loss: 0.0532 - accuracy: 0.9843 - val_loss: 0.0981 - val_accuracy: 0.9595\n",
      "Epoch 4/100\n",
      "4843/4843 [==============================] - 12s 2ms/sample - loss: 0.0394 - accuracy: 0.9878 - val_loss: 0.4792 - val_accuracy: 0.8778\n",
      "Epoch 5/100\n",
      "4843/4843 [==============================] - 12s 2ms/sample - loss: 0.0463 - accuracy: 0.9853 - val_loss: 0.1434 - val_accuracy: 0.9414\n",
      "Epoch 6/100\n",
      "4843/4843 [==============================] - 12s 2ms/sample - loss: 0.0266 - accuracy: 0.9909 - val_loss: 0.2838 - val_accuracy: 0.9331\n",
      "Epoch 7/100\n",
      "4843/4843 [==============================] - 12s 2ms/sample - loss: 0.0250 - accuracy: 0.9915 - val_loss: 0.1063 - val_accuracy: 0.9604\n",
      "Epoch 8/100\n",
      "4843/4843 [==============================] - 12s 2ms/sample - loss: 0.0120 - accuracy: 0.9969 - val_loss: 0.0910 - val_accuracy: 0.9694\n",
      "Epoch 9/100\n",
      "4843/4843 [==============================] - 12s 2ms/sample - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.1247 - val_accuracy: 0.9637\n",
      "Epoch 10/100\n",
      "4843/4843 [==============================] - 12s 2ms/sample - loss: 0.0081 - accuracy: 0.9977 - val_loss: 0.1556 - val_accuracy: 0.9604\n",
      "Epoch 11/100\n",
      "4843/4843 [==============================] - 12s 2ms/sample - loss: 0.0088 - accuracy: 0.9979 - val_loss: 0.1982 - val_accuracy: 0.9554\n",
      "Epoch 12/100\n",
      "4843/4843 [==============================] - 12s 2ms/sample - loss: 0.0130 - accuracy: 0.9957 - val_loss: 0.1043 - val_accuracy: 0.9637\n",
      "Epoch 13/100\n",
      "4843/4843 [==============================] - 11s 2ms/sample - loss: 0.0123 - accuracy: 0.9973 - val_loss: 0.1988 - val_accuracy: 0.9348\n",
      "Training completed in time:  0:02:33.017108\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "\n",
    "earlystopping = EarlyStopping(monitor =\"val_loss\", mode =\"min\", patience = 5, restore_best_weights = True)\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs,callbacks=[earlystopping], validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9377565\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = model.evaluate(X_test,y_test,verbose=0)\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(np.argmax(y_test,axis=-1), np.argmax(y_preds,axis=-1))\n",
    "p = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rumble'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder.inverse_transform([1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94       757\n",
      "           1       0.99      0.88      0.93       705\n",
      "\n",
      "    accuracy                           0.94      1462\n",
      "   macro avg       0.94      0.94      0.94      1462\n",
      "weighted avg       0.94      0.94      0.94      1462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.argmax(y_test,axis=1),np.argmax(y_preds,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSavingPath = 'saved_models/elephant_rumble_model_1.h5'\n",
    "if os.path.isfile(modelSavingPath) is False:\n",
    "    model.save(modelSavingPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelJsonSavingPath = 'saved_models/elephant_rumble_json_architecture_1.json'\n",
    "\n",
    "# serialize model to json\n",
    "json_model = model.to_json()\n",
    "\n",
    "#save the model architecture to JSON file\n",
    "with open(modelJsonSavingPath, 'w') as json_file:\n",
    "    json_file.write(json_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FinalGFCC.ipynb",
   "provenance": []
  },
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
