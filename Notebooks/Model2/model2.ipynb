{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import copy\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gunShotDataPath = './Data/gunShots'\n",
    "speechDataPath = './Data/newHumanSpeech'\n",
    "chainSawDataPath = './Data/newChainSaw'\n",
    "noiseDataPath = '../../Mubin/Data/Training/Noise'\n",
    "elephantCallDataPath = './Data/newElephantCalls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted files\n",
    "\n",
    "for folder in [gunShotDataPath,speechDataPath,chainSawDataPath,noiseDataPath,elephantCallDataPath]:\n",
    "    for file in ['.amlignore', '.amlignore.amltmp']:\n",
    "        if os.path.exists(folder+'/'+file):\n",
    "            os.remove(folder+'/'+file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File numbers for folder ./Data/gunShots range from [1, 2001] and \n",
      "\n",
      "File numbers for folder ./Data/newHumanSpeech range from [1, 1572] and \n",
      "\n",
      "File numbers for folder ./Data/newChainSaw range from [1, 1481] and \n",
      "\n",
      "File numbers for folder ../../Mubin/Data/Training/Noise range from [1, 3180] and \n",
      "\n",
      "File numbers for folder ./Data/newElephantCalls range from [1, 95] and \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for folder in [gunShotDataPath,speechDataPath,chainSawDataPath,noiseDataPath,elephantCallDataPath]:\n",
    "\n",
    "    minFileNumber = math.inf\n",
    "    maxFileNumber = -1\n",
    "    for file in next(os.walk(folder))[2]:\n",
    "        number = int(file.split('_')[1].split('.')[0])\n",
    "        \n",
    "        if number < minFileNumber:\n",
    "            minFileNumber = number\n",
    "        elif number > maxFileNumber:\n",
    "            maxFileNumber = number\n",
    "\n",
    "    print('File numbers for folder {} range from [{}, {}] and \\n'.format(folder,minFileNumber,maxFileNumber))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elephant PreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedData_Elephant = []\n",
    "elephantTag = []\n",
    "\n",
    "for file in next(os.walk(elephantCallDataPath))[2]:\n",
    "\n",
    "    audio,sr = librosa.load(elephantCallDataPath+'/'+file)\n",
    "\n",
    "    # 2seconds = sr*2\n",
    "    target = sr*2\n",
    "\n",
    "    if len(audio) <= target:\n",
    "        newAudio = np.append(audio,np.zeros(target-len(audio)))\n",
    "        \n",
    "        processedData_Elephant.append(newAudio)\n",
    "        elephantTag.append('elephant_call')\n",
    "    else:\n",
    "        audioFileLengthInseconds = len(audio)//target\n",
    "        counter = 0\n",
    "        for i in range(audioFileLengthInseconds):\n",
    "            newAudio = audio[counter : counter+target]\n",
    "            processedData_Elephant.append(newAudio)\n",
    "            elephantTag.append('elephant_call')\n",
    "            counter += target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1443"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processedData_Elephant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcess GunShots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedData_GunShots = []\n",
    "gunShotTag = []\n",
    "\n",
    "for file in next(os.walk(gunShotDataPath))[2]:\n",
    "\n",
    "    audio,sr = librosa.load(gunShotDataPath+'/'+file)\n",
    "\n",
    "    # 2seconds = sr*2\n",
    "    target = sr*2\n",
    "\n",
    "    if len(audio) <= target:\n",
    "        newAudio = np.append(audio,np.zeros(target-len(audio)))\n",
    "        processedData_GunShots.append(newAudio)\n",
    "        gunShotTag.append('gun_shot')\n",
    "    else:\n",
    "        audioFileLengthInseconds = len(audio)//target\n",
    "        counter = 0\n",
    "        for i in range(audioFileLengthInseconds):\n",
    "            newAudio = audio[counter : counter+target]\n",
    "            processedData_GunShots.append(newAudio)\n",
    "            gunShotTag.append('gun_shot')\n",
    "            counter += target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processedData_GunShots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess chainSaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedData_ChainSaw = []\n",
    "chainSawTag = []\n",
    "maxIterations = 2000\n",
    "\n",
    "for file in next(os.walk(chainSawDataPath))[2]:\n",
    "    if maxIterations <= 0:\n",
    "            break\n",
    "    \n",
    "    audio,sr = librosa.load(chainSawDataPath+'/'+file)\n",
    "\n",
    "    # 2seconds = sr*2\n",
    "    target = sr*2\n",
    "\n",
    "    if len(audio) <= target:\n",
    "        newAudio = np.append(audio,np.zeros(target-len(audio)))\n",
    "        processedData_ChainSaw.append(newAudio)\n",
    "        chainSawTag.append('chain_saw')\n",
    "        maxIterations-=1\n",
    "    else:\n",
    "        audioFileLengthInseconds = len(audio)//target\n",
    "        counter = 0\n",
    "        for i in range(audioFileLengthInseconds):\n",
    "            newAudio = audio[counter : counter+target]\n",
    "            processedData_ChainSaw.append(newAudio)\n",
    "            chainSawTag.append('chain_saw')\n",
    "            counter += target\n",
    "        maxIterations -= audioFileLengthInseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processedData_ChainSaw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedData_Speech = []\n",
    "speechTag = []\n",
    "maxIterations = 2000\n",
    "\n",
    "for file in next(os.walk(speechDataPath))[2]:\n",
    "    if maxIterations <= 0:\n",
    "            break\n",
    "    \n",
    "    audio,sr = librosa.load(speechDataPath+'/'+file)\n",
    "\n",
    "    # 2seconds = sr*2\n",
    "    target = sr*2\n",
    "\n",
    "    if len(audio) <= target:\n",
    "        newAudio = np.append(audio,np.zeros(target-len(audio)))\n",
    "        processedData_Speech.append(newAudio)\n",
    "        speechTag.append('human_speech')\n",
    "        maxIterations-=1\n",
    "    else:\n",
    "        audioFileLengthInseconds = len(audio)//target\n",
    "        counter = 0\n",
    "        for i in range(audioFileLengthInseconds):\n",
    "            newAudio = audio[counter : counter+target]\n",
    "            processedData_Speech.append(newAudio)\n",
    "            speechTag.append('human_speech')\n",
    "            counter += target\n",
    "        maxIterations -= audioFileLengthInseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2003"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processedData_Speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedData_Noise = []\n",
    "noiseTag = []\n",
    "maxIterations = 2000\n",
    "\n",
    "for file in next(os.walk(noiseDataPath))[2]:\n",
    "    if maxIterations <= 0:\n",
    "            break\n",
    "    \n",
    "    audio,sr = librosa.load(noiseDataPath+'/'+file)\n",
    "\n",
    "    # 2seconds = sr*2\n",
    "    target = sr*2\n",
    "\n",
    "    if len(audio) <= target:\n",
    "        newAudio = np.append(audio,np.zeros(target-len(audio)))\n",
    "        processedData_Noise.append(newAudio)\n",
    "        noiseTag.append('noise')\n",
    "        maxIterations-=1\n",
    "    else:\n",
    "        audioFileLengthInseconds = len(audio)//target\n",
    "        counter = 0\n",
    "        for i in range(audioFileLengthInseconds):\n",
    "            newAudio = audio[counter : counter+target]\n",
    "            processedData_Noise.append(newAudio)\n",
    "            noiseTag.append('noise')\n",
    "            counter += target\n",
    "        maxIterations -= audioFileLengthInseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processedData_Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalFeatures = 1443\n",
    "\n",
    "final_ChainSaw,finalchainSawTag = processedData_ChainSaw[:totalFeatures],chainSawTag[:totalFeatures]\n",
    "final_Noise,finalnoiseTag = processedData_Noise[:totalFeatures],noiseTag[:totalFeatures]\n",
    "final_Speech,finalspeechTag = processedData_Speech[:totalFeatures],speechTag[:totalFeatures]\n",
    "final_GunShots,finalgunShotTag = processedData_GunShots[:totalFeatures],gunShotTag[:totalFeatures]\n",
    "final_Elephant,finalelephantTag = processedData_Elephant[:totalFeatures],elephantTag[:totalFeatures]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioData = final_ChainSaw + final_Noise + final_Speech + final_GunShots + final_Elephant\n",
    "audioTag = finalchainSawTag + finalnoiseTag + finalspeechTag + finalgunShotTag + finalelephantTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(audioData, audioTag)), columns = ['audioData', 'audioTag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "human_speech     1443\n",
       "elephant_call    1443\n",
       "chain_saw        1443\n",
       "gun_shot         1443\n",
       "noise            1443\n",
       "Name: audioTag, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['audioTag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audioData</th>\n",
       "      <th>audioTag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.054001212, -0.061601162, -0.06775653, -0.0...</td>\n",
       "      <td>chain_saw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.081003785, 0.06602049, 0.04859388, 0.038290...</td>\n",
       "      <td>chain_saw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.00041639805, -0.01920247, -0.038021803, -0....</td>\n",
       "      <td>chain_saw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.25337064, -0.18256235, -0.16470349, -0.227...</td>\n",
       "      <td>chain_saw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0021185875, -0.008524895, -0.009134769, 0.0...</td>\n",
       "      <td>chain_saw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210</th>\n",
       "      <td>[2.2838903532829136e-06, 4.770441137225134e-06...</td>\n",
       "      <td>elephant_call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7211</th>\n",
       "      <td>[1.7580403e-05, 4.7709993e-05, -1.426485e-06, ...</td>\n",
       "      <td>elephant_call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7212</th>\n",
       "      <td>[0.0007098784, 0.0005530115, 0.0009571959, 0.0...</td>\n",
       "      <td>elephant_call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7213</th>\n",
       "      <td>[-2.72627e-05, 4.1527313e-05, 7.598058e-05, -9...</td>\n",
       "      <td>elephant_call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7214</th>\n",
       "      <td>[0.11753148, 0.088640705, 0.056255713, 0.04536...</td>\n",
       "      <td>elephant_call</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7215 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              audioData       audioTag\n",
       "0     [-0.054001212, -0.061601162, -0.06775653, -0.0...      chain_saw\n",
       "1     [0.081003785, 0.06602049, 0.04859388, 0.038290...      chain_saw\n",
       "2     [0.00041639805, -0.01920247, -0.038021803, -0....      chain_saw\n",
       "3     [-0.25337064, -0.18256235, -0.16470349, -0.227...      chain_saw\n",
       "4     [0.0021185875, -0.008524895, -0.009134769, 0.0...      chain_saw\n",
       "...                                                 ...            ...\n",
       "7210  [2.2838903532829136e-06, 4.770441137225134e-06...  elephant_call\n",
       "7211  [1.7580403e-05, 4.7709993e-05, -1.426485e-06, ...  elephant_call\n",
       "7212  [0.0007098784, 0.0005530115, 0.0009571959, 0.0...  elephant_call\n",
       "7213  [-2.72627e-05, 4.1527313e-05, 7.598058e-05, -9...  elephant_call\n",
       "7214  [0.11753148, 0.088640705, 0.056255713, 0.04536...  elephant_call\n",
       "\n",
       "[7215 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['audioData'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(df['audioData'].tolist())\n",
    "y=np.array(df['audioTag'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7215, 44100), (7215,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X.npy', X)\n",
    "np.save('y.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip preprocessing and use npy files instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess shotgun files for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "shotGunDataPath = './Data/shotgun'\n",
    "processedData_shotGun = []\n",
    "shotGunTag = []\n",
    "\n",
    "for file in next(os.walk(shotGunDataPath))[2]:\n",
    "    if file not in ['.amlignore', '.amlignore.amltmp']:\n",
    "\n",
    "        audio,sr = librosa.load(shotGunDataPath+'/'+file)\n",
    "\n",
    "        # 2seconds = sr*2\n",
    "        target = sr*2\n",
    "\n",
    "        if len(audio) <= target:\n",
    "            newAudio = np.append(audio,np.zeros(target-len(audio)))\n",
    "            processedData_shotGun.append(newAudio)\n",
    "            shotGunTag.append('gun_shot')\n",
    "        else:\n",
    "            audioFileLengthInseconds = len(audio)//target\n",
    "            counter = 0\n",
    "            for i in range(audioFileLengthInseconds):\n",
    "                newAudio = audio[counter : counter+target]\n",
    "                processedData_shotGun.append(newAudio)\n",
    "                shotGunTag.append('gun_shot')\n",
    "                counter += target\n",
    "\n",
    "processedData_shotGun = np.array(processedData_shotGun)\n",
    "shotGunTag = np.array(shotGunTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('X.npy')\n",
    "y = np.load('y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave Human Speech Class from X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7215, 44100), (7215,))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before deletion of human speech class:\n",
      "\n",
      "Class: chain_saw count -> 1443\n",
      "Class: elephant_call count -> 1443\n",
      "Class: gun_shot count -> 1443\n",
      "Class: human_speech count -> 1443\n",
      "Class: noise count -> 1443\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print('Before deletion of human speech class:\\n')\n",
    "\n",
    "for i in range(len(unique)):\n",
    "    print('Class: {} count -> {}'.format(unique[i],counts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "tupleList = tuple(list(np.where(y=='human_speech'))[0].tolist())\n",
    "X = np.delete(X, tupleList, 0)\n",
    "y = np.delete(y, tupleList, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deletion of human speech class:\n",
      "\n",
      "Class: chain_saw count -> 1443\n",
      "Class: elephant_call count -> 1443\n",
      "Class: gun_shot count -> 1443\n",
      "Class: noise count -> 1443\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print('After deletion of human speech class:\\n')\n",
    "\n",
    "for i in range(len(unique)):\n",
    "    print('Class: {} count -> {}'.format(unique[i],counts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5772, 44100), (5772,))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes before: 4617 1155 4617 1155\n"
     ]
    }
   ],
   "source": [
    "print('Shapes before: {} {} {} {}'.format(len(X_train), len(X_test), len(y_train), len(y_test)))\n",
    "\n",
    "X_train=np.append(X_train, np.array(processedData_shotGun[:20]) ,axis=0)\n",
    "X_test=np.append(X_test, np.array(processedData_shotGun[20:]) ,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes after: 4637 1164 4637 1164\n"
     ]
    }
   ],
   "source": [
    "y_train = np.append(y_train,shotGunTag[:20])\n",
    "y_test = np.append(y_test,shotGunTag[20:])\n",
    "print('Shapes after: {} {} {} {}'.format(len(X_train), len(X_test), len(y_train), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44100,)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mfcc_features = []\n",
    "X_test_mfcc_features = []\n",
    "numberOfMFCC = 13\n",
    "\n",
    "for i in X_train:\n",
    "    X_train_mfcc_features.append(librosa.feature.mfcc(y=i, n_mfcc=numberOfMFCC))\n",
    "\n",
    "for i in X_test:\n",
    "    X_test_mfcc_features.append(librosa.feature.mfcc(y=i, n_mfcc=numberOfMFCC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train_mfcc_features)\n",
    "X_test = np.array(X_test_mfcc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder=LabelEncoder()\n",
    "y_train = to_categorical(labelencoder.fit_transform(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = to_categorical(labelencoder.transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size=0.2, random_state=0, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1,X_train.shape[1],X_train.shape[2],1)\n",
    "\n",
    "X_val = X_val.reshape(-1,X_val.shape[1],X_val.shape[2],1)\n",
    "\n",
    "X_test = X_test.reshape(-1,X_test.shape[1],X_test.shape[2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3709, 13, 87, 1) (3709, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(928, 13, 87, 1) (928, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1164, 13, 87, 1) (1164, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D, Dense, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, input_shape=X_train.shape[1:] ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 11, 85, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 11, 85, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 11, 85, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 5, 42, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 3, 40, 32)         18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 3, 40, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 3, 40, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 1, 20, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 63,108\n",
      "Trainable params: 62,724\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3709 samples, validate on 928 samples\n",
      "Epoch 1/100\n",
      "3709/3709 [==============================] - 5s 1ms/sample - loss: 0.2838 - accuracy: 0.9337 - val_loss: 0.2333 - val_accuracy: 0.9224\n",
      "Epoch 2/100\n",
      "3709/3709 [==============================] - 4s 1ms/sample - loss: 0.1429 - accuracy: 0.9590 - val_loss: 0.1777 - val_accuracy: 0.9397\n",
      "Epoch 3/100\n",
      "3709/3709 [==============================] - 4s 1ms/sample - loss: 0.1106 - accuracy: 0.9636 - val_loss: 0.0879 - val_accuracy: 0.9688\n",
      "Epoch 4/100\n",
      "3709/3709 [==============================] - 4s 995us/sample - loss: 0.0979 - accuracy: 0.9679 - val_loss: 0.0866 - val_accuracy: 0.9720\n",
      "Epoch 5/100\n",
      "3709/3709 [==============================] - 4s 988us/sample - loss: 0.0853 - accuracy: 0.9728 - val_loss: 0.0674 - val_accuracy: 0.9731\n",
      "Epoch 6/100\n",
      "3709/3709 [==============================] - 4s 1ms/sample - loss: 0.0706 - accuracy: 0.9749 - val_loss: 0.0960 - val_accuracy: 0.9698\n",
      "Epoch 7/100\n",
      "3680/3709 [============================>.] - ETA: 0s - loss: 0.0612 - accuracy: 0.9769\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "3709/3709 [==============================] - 4s 1ms/sample - loss: 0.0617 - accuracy: 0.9768 - val_loss: 0.0717 - val_accuracy: 0.9741\n",
      "Epoch 8/100\n",
      "3709/3709 [==============================] - 4s 990us/sample - loss: 0.0423 - accuracy: 0.9841 - val_loss: 0.0565 - val_accuracy: 0.9784\n",
      "Epoch 9/100\n",
      "3709/3709 [==============================] - 4s 1ms/sample - loss: 0.0382 - accuracy: 0.9879 - val_loss: 0.0576 - val_accuracy: 0.9774\n",
      "Epoch 10/100\n",
      "3709/3709 [==============================] - 4s 992us/sample - loss: 0.0342 - accuracy: 0.9895 - val_loss: 0.0557 - val_accuracy: 0.9795\n",
      "Epoch 11/100\n",
      "3709/3709 [==============================] - 4s 986us/sample - loss: 0.0306 - accuracy: 0.9922 - val_loss: 0.0536 - val_accuracy: 0.9806\n",
      "Epoch 12/100\n",
      "3709/3709 [==============================] - 4s 983us/sample - loss: 0.0300 - accuracy: 0.9914 - val_loss: 0.0525 - val_accuracy: 0.9806\n",
      "Epoch 13/100\n",
      "3709/3709 [==============================] - 4s 1ms/sample - loss: 0.0262 - accuracy: 0.9941 - val_loss: 0.0568 - val_accuracy: 0.9784\n",
      "Epoch 14/100\n",
      "3680/3709 [============================>.] - ETA: 0s - loss: 0.0266 - accuracy: 0.9929\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "3709/3709 [==============================] - 4s 994us/sample - loss: 0.0265 - accuracy: 0.9930 - val_loss: 0.0530 - val_accuracy: 0.9817\n",
      "Epoch 15/100\n",
      "3709/3709 [==============================] - 4s 1ms/sample - loss: 0.0238 - accuracy: 0.9930 - val_loss: 0.0523 - val_accuracy: 0.9817\n",
      "Epoch 16/100\n",
      "3709/3709 [==============================] - 4s 1ms/sample - loss: 0.0267 - accuracy: 0.9916 - val_loss: 0.0515 - val_accuracy: 0.9817\n",
      "Epoch 17/100\n",
      "3709/3709 [==============================] - 4s 1ms/sample - loss: 0.0256 - accuracy: 0.9922 - val_loss: 0.0519 - val_accuracy: 0.9838\n",
      "Epoch 18/100\n",
      "3680/3709 [============================>.] - ETA: 0s - loss: 0.0241 - accuracy: 0.9943\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "3709/3709 [==============================] - 4s 1ms/sample - loss: 0.0242 - accuracy: 0.9943 - val_loss: 0.0523 - val_accuracy: 0.9838\n",
      "Epoch 19/100\n",
      "3709/3709 [==============================] - 4s 995us/sample - loss: 0.0237 - accuracy: 0.9933 - val_loss: 0.0525 - val_accuracy: 0.9838\n",
      "Training completed in time:  0:01:11.647121\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor =\"val_loss\", mode =\"min\", patience = 3, restore_best_weights = True),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=2, min_lr=0.00001, verbose=1),\n",
    "]\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs,callbacks=callbacks, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97422683\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = model.evaluate(X_test,y_test,verbose=0)\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 maps to: chain_saw\n",
      "1 maps to: elephant_call\n",
      "2 maps to: gun_shot\n",
      "3 maps to: noise\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print('{} maps to: {}'.format(i,labelencoder.inverse_transform([i])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(np.argmax(y_test,axis=-1), np.argmax(y_preds,axis=-1))\n",
    "\n",
    "ar = ['chain_saw','elephant_call','gun_shot','noise']\n",
    "\n",
    "cm_df = pd.DataFrame(cm,index = ar, columns = ar)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(cm_df, annot=True, fmt='g')\n",
    "plt.title('CNN model\\nAccuracy:{0:.3f}'.format(100*accuracy_score(np.argmax(y_test,axis=-1), np.argmax(y_preds,axis=-1))))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelSavingPath = 'saved_models/model_2.h5'\n",
    "# if os.path.isfile(modelSavingPath) is False:\n",
    "#     model.save(modelSavingPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelJsonSavingPath = 'saved_models/model_2.json'\n",
    "\n",
    "# # serialize model to json\n",
    "# json_model = model.to_json()\n",
    "\n",
    "# #save the model architecture to JSON file\n",
    "# with open(modelJsonSavingPath, 'w') as json_file:\n",
    "#     json_file.write(json_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18.5\n",
      "0.25.3\n",
      "0.8.0\n",
      "2.1.0\n",
      "0.22.2.post1\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(librosa.__version__)\n",
    "print(tf.__version__)\n",
    "print(sk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
